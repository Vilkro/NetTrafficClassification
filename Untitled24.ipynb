{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1hJi4hAth0c3c1CG3Is_7wV7pqDOTQJfb",
      "authorship_tag": "ABX9TyOOUbFkL6iR3mlCvoHuB22F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vilkro/NetTrafficClassification/blob/main/Untitled24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1WWdRFORpNc",
        "outputId": "223dd41d-4cbb-46a9-a623-8798a41d6cda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import math\n",
        "\n",
        "base_path = \"/content/drive/MyDrive/CIC-IDS/CIC-IDS-2017/Dataset/CIC-IDS-2017/CSVs/MachineLearningCSV/MachineLearningCVE\"\n",
        "\n",
        "csv_files = [\n",
        "    \"Monday-WorkingHours.pcap_ISCX.csv\",\n",
        "    \"Tuesday-WorkingHours.pcap_ISCX.csv\",\n",
        "    \"Wednesday-workingHours.pcap_ISCX.csv\",\n",
        "    \"Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\",\n",
        "    \"Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\",\n",
        "    \"Friday-WorkingHours-Morning.pcap_ISCX.csv\",\n",
        "    \"Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\",\n",
        "    \"Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\"\n",
        "]\n",
        "\n",
        "# =====================================================\n",
        "# 0. LOAD RAW DATA\n",
        "# =====================================================\n",
        "dfs = [pd.read_csv(os.path.join(base_path, f)) for f in csv_files]\n",
        "data = pd.concat(dfs, ignore_index=True)\n",
        "data.columns = data.columns.str.strip()\n",
        "\n",
        "# =====================================================\n",
        "# 1. CLEAN INF / NAN\n",
        "# =====================================================\n",
        "data = data.replace([np.inf, -np.inf], np.nan)\n",
        "data = data.dropna()\n",
        "\n",
        "# =====================================================\n",
        "# 2. MAP LABELS → 5 CLASSES\n",
        "# =====================================================\n",
        "def map_label(l):\n",
        "    l = str(l).strip()\n",
        "    if l == \"BENIGN\": return \"Benign\"\n",
        "    if \"DoS\" in l or \"DDoS\" in l: return \"DoS/DDoS\"\n",
        "    if \"PortScan\" in l: return \"PortScan\"\n",
        "    if \"Patator\" in l or \"Brute Force\" in l: return \"Brute Force\"\n",
        "    if \"Web Attack\" in l: return \"Web Attacks\"\n",
        "    return None\n",
        "\n",
        "data[\"Class\"] = data[\"Label\"].apply(map_label)\n",
        "data = data[data[\"Class\"].notna()]  # drop unknowns\n",
        "\n",
        "print(\"Labels mapped:\", data[\"Class\"].value_counts())\n",
        "\n",
        "# =====================================================\n",
        "# 2.5 BALANCED DOWNSAMPLING (to prevent RAM crash)\n",
        "# =====================================================\n",
        "\n",
        "MAX_SAMPLES_PER_CLASS = 60000  # try 40k or 20k if still heavy\n",
        "\n",
        "sampled_data = []\n",
        "\n",
        "for cls in data[\"Class\"].unique():\n",
        "    df_cls = data[data[\"Class\"] == cls]\n",
        "\n",
        "    if len(df_cls) > MAX_SAMPLES_PER_CLASS:\n",
        "        df_cls = df_cls.sample(MAX_SAMPLES_PER_CLASS, random_state=42)\n",
        "\n",
        "    sampled_data.append(df_cls)\n",
        "\n",
        "data = pd.concat(sampled_data, ignore_index=True)\n",
        "\n",
        "print(\"After balanced sampling:\", data.shape)\n",
        "print(data[\"Class\"].value_counts())\n",
        "\n",
        "# =====================================================\n",
        "# 3. DROP NON-NUMERIC COLUMNS\n",
        "# =====================================================\n",
        "cols_to_drop = [\"Label\", \"Timestamp\", \"Flow ID\", \"Source IP\",\n",
        "                \"Destination IP\", \"Source Port\", \"Destination Port\", \"Protocol\"]\n",
        "\n",
        "for c in cols_to_drop:\n",
        "    if c in data.columns:\n",
        "        data = data.drop(columns=[c])\n",
        "\n",
        "# =====================================================\n",
        "# 4. EXTRACT X, y\n",
        "# =====================================================\n",
        "X = data.select_dtypes(include=[\"float64\", \"int64\"])\n",
        "y = data[\"Class\"]\n",
        "\n",
        "X = X.replace([np.inf, -np.inf], np.nan)\n",
        "mask = X.notna().all(axis=1)\n",
        "X = X[mask]\n",
        "y = y[mask]\n",
        "\n",
        "# =====================================================\n",
        "# 5. ENCODE TARGET\n",
        "# =====================================================\n",
        "le = LabelEncoder()\n",
        "y_enc = le.fit_transform(y)\n",
        "num_classes = len(le.classes_)\n",
        "\n",
        "# =====================================================\n",
        "# 6. TRAIN/VAL/TEST SPLIT\n",
        "# =====================================================\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y_enc, test_size=0.30, random_state=42, stratify=y_enc\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 7. STANDARDIZE\n",
        "# =====================================================\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_val_s   = scaler.transform(X_val)\n",
        "X_test_s  = scaler.transform(X_test)\n",
        "\n",
        "print(\"Standardized shapes:\", X_train_s.shape, X_val_s.shape, X_test_s.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBw_vdlLSis0",
        "outputId": "562c5536-01ba-4ffb-d938-a7008d460ba3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels mapped: Class\n",
            "Benign         2271320\n",
            "DoS/DDoS        379737\n",
            "PortScan        158804\n",
            "Brute Force      15339\n",
            "Web Attacks        673\n",
            "Name: count, dtype: int64\n",
            "After balanced sampling: (196012, 80)\n",
            "Class\n",
            "Benign         60000\n",
            "DoS/DDoS       60000\n",
            "PortScan       60000\n",
            "Brute Force    15339\n",
            "Web Attacks      673\n",
            "Name: count, dtype: int64\n",
            "Standardized shapes: (137208, 77) (29402, 77) (29402, 77)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import Birch\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# 1) Fit Birch on TRAIN ONLY\n",
        "n_clusters = 20    # you can tune this (10–50)\n",
        "birch = Birch(n_clusters=n_clusters)\n",
        "birch.fit(X_train_s)\n",
        "\n",
        "# 2) Get cluster labels for train/val/test\n",
        "train_clusters = birch.labels_\n",
        "val_clusters   = birch.predict(X_val_s)\n",
        "test_clusters  = birch.predict(X_test_s)\n",
        "\n",
        "print(\"Cluster label sample:\", np.unique(train_clusters))\n",
        "\n",
        "# 3) One-hot encode cluster IDs\n",
        "ohe = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
        "train_cluster_oh = ohe.fit_transform(train_clusters.reshape(-1, 1))\n",
        "val_cluster_oh   = ohe.transform(val_clusters.reshape(-1, 1))\n",
        "test_cluster_oh  = ohe.transform(test_clusters.reshape(-1, 1))\n",
        "\n",
        "print(\"One-hot shape (train):\", train_cluster_oh.shape)\n",
        "\n",
        "# 4) Concatenate original features + cluster one-hot\n",
        "X_train_birch = np.concatenate([X_train_s, train_cluster_oh], axis=1)\n",
        "X_val_birch   = np.concatenate([X_val_s,   val_cluster_oh],   axis=1)\n",
        "X_test_birch  = np.concatenate([X_test_s,  test_cluster_oh],  axis=1)\n",
        "\n",
        "input_dim_birch = X_train_birch.shape[1]\n",
        "num_classes     = len(le.classes_)\n",
        "\n",
        "print(\"Birch hybrid shapes:\")\n",
        "print(\"Train:\", X_train_birch.shape)\n",
        "print(\"Val:  \", X_val_birch.shape)\n",
        "print(\"Test: \", X_test_birch.shape)\n",
        "print(\"Input dim birch:\", input_dim_birch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mp_TzkZf1zoC",
        "outputId": "b8c99f4d-2551-4c7c-d5ca-440ce4b123bd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster label sample: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
            "One-hot shape (train): (137208, 20)\n",
            "Birch hybrid shapes:\n",
            "Train: (137208, 97)\n",
            "Val:   (29402, 97)\n",
            "Test:  (29402, 97)\n",
            "Input dim birch: 97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import math\n",
        "\n",
        "def build_cldnn(input_dim, num_classes):\n",
        "    m = models.Sequential([\n",
        "        layers.Input(shape=(input_dim,)),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.BatchNormalization(), layers.Dropout(0.3),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.BatchNormalization(), layers.Dropout(0.3),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    m.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "    return m\n",
        "\n",
        "def build_1d_cnn(input_dim, num_classes):\n",
        "    m = models.Sequential([\n",
        "        layers.Reshape((input_dim, 1)),\n",
        "        layers.Conv1D(64, 3, activation='relu'),\n",
        "        layers.MaxPooling1D(2),\n",
        "        layers.Conv1D(128, 3, activation='relu'),\n",
        "        layers.GlobalMaxPooling1D(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    m.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "    return m\n",
        "\n",
        "def build_cnn(input_dim, num_classes):\n",
        "    side = int(math.ceil(math.sqrt(input_dim)))\n",
        "    pad = side*side - input_dim\n",
        "    m = models.Sequential([\n",
        "        layers.Input(shape=(input_dim,)),\n",
        "        layers.Lambda(lambda x: tf.pad(x, [[0,0],[0,pad]])),\n",
        "        layers.Reshape((side, side, 1)),\n",
        "        layers.Conv2D(32, (3,3), activation='relu'),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Conv2D(64, (3,3), activation='relu'),\n",
        "        layers.GlobalMaxPooling2D(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    m.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "    return m\n",
        "\n",
        "def build_cnn_lstm(input_dim, num_classes):\n",
        "    # simplified version that treats features as a sequence\n",
        "    m = models.Sequential([\n",
        "        layers.Input(shape=(input_dim,)),\n",
        "        layers.Reshape((input_dim, 1)),\n",
        "        layers.Conv1D(32, kernel_size=3, activation='relu'),\n",
        "        layers.MaxPooling1D(pool_size=2),\n",
        "        layers.LSTM(64),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    m.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "    return m\n"
      ],
      "metadata": {
        "id": "B8YaelIe2L3p"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run(model, name, X_tr, X_v, X_te):\n",
        "    print(f\"\\n===== {name} =====\")\n",
        "    history = model.fit(\n",
        "        X_tr, y_train,\n",
        "        validation_data=(X_v, y_val),\n",
        "        epochs=5,\n",
        "        batch_size=64,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # precise metrics\n",
        "    val_acc  = history.history[\"val_accuracy\"][-1]\n",
        "    val_loss = history.history[\"val_loss\"][-1]\n",
        "    print(f\"Validation Accuracy: {val_acc:.6f}\")\n",
        "    print(f\"Validation Loss:     {val_loss:.6f}\")\n",
        "\n",
        "    loss, acc = model.evaluate(X_te, y_test, verbose=0)\n",
        "    print(f\"Test Accuracy:       {acc:.6f}\")\n",
        "    print(f\"Test Loss:           {loss:.6f}\")\n",
        "\n",
        "    preds = model.predict(X_te).argmax(axis=1)\n",
        "\n",
        "    print(classification_report(\n",
        "        y_test,\n",
        "        preds,\n",
        "        target_names=le.classes_,\n",
        "        digits=6\n",
        "    ))\n",
        "    print(\"Confusion:\\n\", confusion_matrix(y_test, preds))\n",
        "\n",
        "    return history, preds\n"
      ],
      "metadata": {
        "id": "J-JbQK5j2WsJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Birch + ClDNN\n",
        "cldnn_birch = build_cldnn(input_dim_birch, num_classes)\n",
        "hist_birch_cldnn, preds_birch_cldnn = run(\n",
        "    cldnn_birch, \"Birch + ClDNN\",\n",
        "    X_train_birch, X_val_birch, X_test_birch\n",
        ")\n",
        "\n",
        "# Birch + 1D-CNN\n",
        "cnn1d_birch = build_1d_cnn(input_dim_birch, num_classes)\n",
        "hist_birch_1dcnn, preds_birch_1dcnn = run(\n",
        "    cnn1d_birch, \"Birch + 1D-CNN\",\n",
        "    X_train_birch, X_val_birch, X_test_birch\n",
        ")\n",
        "\n",
        "# Birch + CNN\n",
        "cnn_birch = build_cnn(input_dim_birch, num_classes)\n",
        "hist_birch_cnn, preds_birch_cnn = run(\n",
        "    cnn_birch, \"Birch + CNN\",\n",
        "    X_train_birch, X_val_birch, X_test_birch\n",
        ")\n",
        "\n",
        "# Birch + CNN-LSTM\n",
        "cnnlstm_birch = build_cnn_lstm(input_dim_birch, num_classes)\n",
        "hist_birch_cnnlstm, preds_birch_cnnlstm = run(\n",
        "    cnnlstm_birch, \"Birch + CNN-LSTM\",\n",
        "    X_train_birch, X_val_birch, X_test_birch\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5eWVE9n7kPs",
        "outputId": "ec71964a-acea-4ae3-c8a0-7e38c335ad55"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Birch + ClDNN =====\n",
            "Epoch 1/5\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 5ms/step - accuracy: 0.9066 - loss: 0.2474 - val_accuracy: 0.9588 - val_loss: 0.1161\n",
            "Epoch 2/5\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9536 - loss: 0.1225 - val_accuracy: 0.9662 - val_loss: 0.0998\n",
            "Epoch 3/5\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9592 - loss: 0.1097 - val_accuracy: 0.9564 - val_loss: 0.1177\n",
            "Epoch 4/5\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9634 - loss: 0.1001 - val_accuracy: 0.9685 - val_loss: 0.0892\n",
            "Epoch 5/5\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9636 - loss: 0.0996 - val_accuracy: 0.9662 - val_loss: 0.0981\n",
            "Validation Accuracy: 0.966159\n",
            "Validation Loss:     0.098137\n",
            "Test Accuracy:       0.966839\n",
            "Test Loss:           0.096519\n",
            "\u001b[1m919/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Benign   0.990975  0.915000  0.951473      9000\n",
            " Brute Force   0.842610  0.993481  0.911847      2301\n",
            "    DoS/DDoS   0.964324  0.991111  0.977534      9000\n",
            "    PortScan   0.984336  0.998444  0.991340      9000\n",
            " Web Attacks   0.000000  0.000000  0.000000       101\n",
            "\n",
            "    accuracy                       0.966839     29402\n",
            "   macro avg   0.756449  0.779607  0.766439     29402\n",
            "weighted avg   0.965770  0.966839  0.965284     29402\n",
            "\n",
            "Confusion:\n",
            " [[8235  314  310  141    0]\n",
            " [  10 2286    3    2    0]\n",
            " [  63   17 8920    0    0]\n",
            " [   0    2   12 8986    0]\n",
            " [   2   94    5    0    0]]\n",
            "\n",
            "===== Birch + 1D-CNN =====\n",
            "Epoch 1/5\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.8650 - loss: 0.3832 - val_accuracy: 0.9556 - val_loss: 0.1452\n",
            "Epoch 2/5\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9492 - loss: 0.1440 - val_accuracy: 0.9445 - val_loss: 0.1319\n",
            "Epoch 3/5\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9557 - loss: 0.1234 - val_accuracy: 0.9548 - val_loss: 0.1420\n",
            "Epoch 4/5\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9581 - loss: 0.1204 - val_accuracy: 0.9618 - val_loss: 0.1114\n",
            "Epoch 5/5\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9630 - loss: 0.1060 - val_accuracy: 0.9671 - val_loss: 0.1006\n",
            "Validation Accuracy: 0.967077\n",
            "Validation Loss:     0.100622\n",
            "Test Accuracy:       0.965649\n",
            "Test Loss:           0.098779\n",
            "\u001b[1m919/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Benign   0.989150  0.911667  0.948829      9000\n",
            " Brute Force   0.857692  0.969144  0.910018      2301\n",
            "    DoS/DDoS   0.956471  0.996111  0.975889      9000\n",
            "    PortScan   0.984445  0.998556  0.991450      9000\n",
            " Web Attacks   1.000000  0.049505  0.094340       101\n",
            "\n",
            "    accuracy                       0.965649     29402\n",
            "   macro avg   0.957552  0.784996  0.784105     29402\n",
            "weighted avg   0.967456  0.965649  0.964186     29402\n",
            "\n",
            "Confusion:\n",
            " [[8205  278  380  137    0]\n",
            " [  55 2230   14    2    0]\n",
            " [  32    1 8965    2    0]\n",
            " [   2    2    9 8987    0]\n",
            " [   1   89    5    1    5]]\n",
            "\n",
            "===== Birch + CNN =====\n",
            "Epoch 1/5\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.8831 - loss: 0.3199 - val_accuracy: 0.9520 - val_loss: 0.1325\n",
            "Epoch 2/5\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9517 - loss: 0.1309 - val_accuracy: 0.9666 - val_loss: 0.1061\n",
            "Epoch 3/5\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9617 - loss: 0.1054 - val_accuracy: 0.9655 - val_loss: 0.1029\n",
            "Epoch 4/5\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9640 - loss: 0.1005 - val_accuracy: 0.9654 - val_loss: 0.0997\n",
            "Epoch 5/5\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9645 - loss: 0.0980 - val_accuracy: 0.9687 - val_loss: 0.0898\n",
            "Validation Accuracy: 0.968744\n",
            "Validation Loss:     0.089785\n",
            "Test Accuracy:       0.968336\n",
            "Test Loss:           4.351017\n",
            "\u001b[1m919/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Benign   0.976506  0.932889  0.954199      9000\n",
            " Brute Force   0.889163  0.941330  0.914503      2301\n",
            "    DoS/DDoS   0.965368  0.991111  0.978070      9000\n",
            "    PortScan   0.984765  0.998333  0.991503      9000\n",
            " Web Attacks   1.000000  0.039604  0.076190       101\n",
            "\n",
            "    accuracy                       0.968335     29402\n",
            "   macro avg   0.963160  0.780653  0.782893     29402\n",
            "weighted avg   0.968870  0.968335  0.966802     29402\n",
            "\n",
            "Confusion:\n",
            " [[8396  163  303  138    0]\n",
            " [ 131 2166    4    0    0]\n",
            " [  64   16 8920    0    0]\n",
            " [   2    2   11 8985    0]\n",
            " [   5   89    2    1    4]]\n",
            "\n",
            "===== Birch + CNN-LSTM =====\n",
            "Epoch 1/5\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 8ms/step - accuracy: 0.7277 - loss: 0.7625 - val_accuracy: 0.8815 - val_loss: 0.3272\n",
            "Epoch 2/5\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.8945 - loss: 0.2731 - val_accuracy: 0.9215 - val_loss: 0.2015\n",
            "Epoch 3/5\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.9264 - loss: 0.1830 - val_accuracy: 0.9351 - val_loss: 0.1622\n",
            "Epoch 4/5\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - accuracy: 0.9340 - loss: 0.1632 - val_accuracy: 0.9339 - val_loss: 0.1638\n",
            "Epoch 5/5\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.9407 - loss: 0.1488 - val_accuracy: 0.9515 - val_loss: 0.1379\n",
            "Validation Accuracy: 0.951534\n",
            "Validation Loss:     0.137944\n",
            "Test Accuracy:       0.951704\n",
            "Test Loss:           0.136261\n",
            "\u001b[1m919/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Benign   0.978045  0.876111  0.924276      9000\n",
            " Brute Force   0.801497  0.977401  0.880752      2301\n",
            "    DoS/DDoS   0.944800  0.985111  0.964534      9000\n",
            "    PortScan   0.981639  0.998000  0.989752      9000\n",
            " Web Attacks   0.000000  0.000000  0.000000       101\n",
            "\n",
            "    accuracy                       0.951704     29402\n",
            "   macro avg   0.741196  0.767325  0.751863     29402\n",
            "weighted avg   0.951792  0.951704  0.950060     29402\n",
            "\n",
            "Confusion:\n",
            " [[7885  451  499  165    0]\n",
            " [  47 2249    3    2    0]\n",
            " [ 121   13 8866    0    0]\n",
            " [   3    4   11 8982    0]\n",
            " [   6   89    5    1    0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}